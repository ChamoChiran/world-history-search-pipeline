{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6235f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter details: CHAPTER: None - None | pg-10\n",
      "text snippet: '1 Eugene Berger. 1.1 Chronology. 8 – 6 million years ago Bi-pedal hominids in Africa 2.6 million yea'\n",
      "chapter details: CHAPTER: None - None | pg-11\n",
      "text snippet: 'not content with simply reaching remote places; they were curious about their earliest human inhabit'\n",
      "chapter details: CHAPTER: 1 - PREHISTORY | pg-12\n",
      "text snippet: 'CHAPTER 1: PREHISTORY 1.3 QUESTIONS TO GUIDE YOUR READING 1. What were some factors that led to homi'\n",
      "chapter details: CHAPTER: 1 - PREHISTORY | pg-13\n",
      "text snippet: 'some time. While some bipedal hominids may have stayed in the forest, climate changes did drive othe'\n",
      "chapter details: CHAPTER: 1 - PREHISTORY | pg-14\n",
      "text snippet: 'CHAPTER 1: PREHISTORY major effects on hominid development. First, with sea levels dropping due to g'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/clean/cleaned_book.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for i, (key, value) in enumerate(data.items()):\n",
    "    \n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "    print(f\"chapter details: {value['chapter_details']}\")\n",
    "    print(f\"text snippet: {value['text'][:100]!r}\")\n",
    "\n",
    "# Create chapter_texts dictionary: {chapter_details: text}\n",
    "chapter_texts = {}\n",
    "for key, value in data.items():\n",
    "    chapter_details = value['chapter_details']\n",
    "    text = value['text']\n",
    "    \n",
    "    # Aggregate text by chapter\n",
    "    if chapter_details not in chapter_texts:\n",
    "        chapter_texts[chapter_details] = []\n",
    "    chapter_texts[chapter_details].append(text)\n",
    "\n",
    "# Join all page texts for each chapter\n",
    "chapter_texts = {ch: \" \".join(texts) for ch, texts in chapter_texts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fb1f6",
   "metadata": {},
   "source": [
    "# NLTK tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8333d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure NLTK models exist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def split_sentences(pages):\n",
    "    \"\"\"Add a 'sentences' list to each page.\"\"\"\n",
    "    for key, value in pages.items():\n",
    "        text = value.get(\"text\", \"\").strip()\n",
    "\n",
    "        if text:\n",
    "            sentences = sent_tokenize(text)\n",
    "        else:\n",
    "            sentences = []\n",
    "\n",
    "        value[\"sentences\"] = sentences\n",
    "\n",
    "    return pages\n",
    "chapter_sentences = split_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef675d9",
   "metadata": {},
   "source": [
    "# MiniLM to create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be6b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Chunks: 7353\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import uuid\n",
    "\n",
    "def build_semantic_chunks(pages,\n",
    "                          model_name=\"all-MiniLM-L6-v2\",\n",
    "                          similarity_threshold=0.55,\n",
    "                          max_sentences_per_chunk=10):\n",
    "\n",
    "    print(\"Loading embedding model...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for page_num, page in pages.items():\n",
    "        sentences = page.get(\"sentences\", [])\n",
    "        chapter_m = page.get(\"chapter_details\")\n",
    "\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # Embed page sentences\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        current_chunk_sentences = []\n",
    "        current_chunk_embeddings = []\n",
    "\n",
    "        def flush_chunk():\n",
    "            if not current_chunk_sentences:\n",
    "                return\n",
    "\n",
    "            chunk_text = \" \".join(current_chunk_sentences).strip()\n",
    "            if not chunk_text:\n",
    "                return\n",
    "\n",
    "            all_chunks.append({\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"chapter_metadata\": chapter_m,\n",
    "                \"text\": chunk_text,\n",
    "            })\n",
    "\n",
    "        # Build chunks\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sent_emb = embeddings[i]\n",
    "\n",
    "            if not current_chunk_embeddings:\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                current_chunk_embeddings.append(sent_emb)\n",
    "                continue\n",
    "\n",
    "            prev_emb = current_chunk_embeddings[-1]\n",
    "            similarity = util.pytorch_cos_sim(prev_emb, sent_emb).item()\n",
    "\n",
    "            split_by_size = len(current_chunk_sentences) >= max_sentences_per_chunk\n",
    "            split_by_similarity = similarity < similarity_threshold\n",
    "\n",
    "            if split_by_size or split_by_similarity:\n",
    "                flush_chunk()\n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_chunk_embeddings = [sent_emb]\n",
    "            else:\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                current_chunk_embeddings.append(sent_emb)\n",
    "\n",
    "        # Flush any remaining chunk\n",
    "        flush_chunk()\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "chunks = build_semantic_chunks(chapter_sentences)\n",
    "print(\"Chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422af889",
   "metadata": {},
   "source": [
    "# Build chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf8c1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embedder(model_name=\"all-MiniLM-L6-v2\"):\n",
    "    return SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e6cb831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored batch 1: 100/7353 chunks\n",
      "Stored batch 2: 200/7353 chunks\n",
      "Stored batch 3: 300/7353 chunks\n",
      "Stored batch 4: 400/7353 chunks\n",
      "Stored batch 5: 500/7353 chunks\n",
      "Stored batch 6: 600/7353 chunks\n",
      "Stored batch 7: 700/7353 chunks\n",
      "Stored batch 5: 500/7353 chunks\n",
      "Stored batch 6: 600/7353 chunks\n",
      "Stored batch 7: 700/7353 chunks\n",
      "Stored batch 8: 800/7353 chunks\n",
      "Stored batch 9: 900/7353 chunks\n",
      "Stored batch 10: 1000/7353 chunks\n",
      "Stored batch 8: 800/7353 chunks\n",
      "Stored batch 9: 900/7353 chunks\n",
      "Stored batch 10: 1000/7353 chunks\n",
      "Stored batch 11: 1100/7353 chunks\n",
      "Stored batch 12: 1200/7353 chunks\n",
      "Stored batch 13: 1300/7353 chunks\n",
      "Stored batch 14: 1400/7353 chunks\n",
      "Stored batch 11: 1100/7353 chunks\n",
      "Stored batch 12: 1200/7353 chunks\n",
      "Stored batch 13: 1300/7353 chunks\n",
      "Stored batch 14: 1400/7353 chunks\n",
      "Stored batch 15: 1500/7353 chunks\n",
      "Stored batch 16: 1600/7353 chunks\n",
      "Stored batch 17: 1700/7353 chunks\n",
      "Stored batch 15: 1500/7353 chunks\n",
      "Stored batch 16: 1600/7353 chunks\n",
      "Stored batch 17: 1700/7353 chunks\n",
      "Stored batch 18: 1800/7353 chunks\n",
      "Stored batch 19: 1900/7353 chunks\n",
      "Stored batch 20: 2000/7353 chunks\n",
      "Stored batch 18: 1800/7353 chunks\n",
      "Stored batch 19: 1900/7353 chunks\n",
      "Stored batch 20: 2000/7353 chunks\n",
      "Stored batch 21: 2100/7353 chunks\n",
      "Stored batch 22: 2200/7353 chunks\n",
      "Stored batch 23: 2300/7353 chunks\n",
      "Stored batch 21: 2100/7353 chunks\n",
      "Stored batch 22: 2200/7353 chunks\n",
      "Stored batch 23: 2300/7353 chunks\n",
      "Stored batch 24: 2400/7353 chunks\n",
      "Stored batch 25: 2500/7353 chunks\n",
      "Stored batch 26: 2600/7353 chunks\n",
      "Stored batch 24: 2400/7353 chunks\n",
      "Stored batch 25: 2500/7353 chunks\n",
      "Stored batch 26: 2600/7353 chunks\n",
      "Stored batch 27: 2700/7353 chunks\n",
      "Stored batch 28: 2800/7353 chunks\n",
      "Stored batch 29: 2900/7353 chunks\n",
      "Stored batch 27: 2700/7353 chunks\n",
      "Stored batch 28: 2800/7353 chunks\n",
      "Stored batch 29: 2900/7353 chunks\n",
      "Stored batch 30: 3000/7353 chunks\n",
      "Stored batch 31: 3100/7353 chunks\n",
      "Stored batch 32: 3200/7353 chunks\n",
      "Stored batch 30: 3000/7353 chunks\n",
      "Stored batch 31: 3100/7353 chunks\n",
      "Stored batch 32: 3200/7353 chunks\n",
      "Stored batch 33: 3300/7353 chunks\n",
      "Stored batch 34: 3400/7353 chunks\n",
      "Stored batch 35: 3500/7353 chunks\n",
      "Stored batch 33: 3300/7353 chunks\n",
      "Stored batch 34: 3400/7353 chunks\n",
      "Stored batch 35: 3500/7353 chunks\n",
      "Stored batch 36: 3600/7353 chunks\n",
      "Stored batch 37: 3700/7353 chunks\n",
      "Stored batch 38: 3800/7353 chunks\n",
      "Stored batch 36: 3600/7353 chunks\n",
      "Stored batch 37: 3700/7353 chunks\n",
      "Stored batch 38: 3800/7353 chunks\n",
      "Stored batch 39: 3900/7353 chunks\n",
      "Stored batch 40: 4000/7353 chunks\n",
      "Stored batch 39: 3900/7353 chunks\n",
      "Stored batch 40: 4000/7353 chunks\n",
      "Stored batch 41: 4100/7353 chunks\n",
      "Stored batch 42: 4200/7353 chunks\n",
      "Stored batch 43: 4300/7353 chunks\n",
      "Stored batch 41: 4100/7353 chunks\n",
      "Stored batch 42: 4200/7353 chunks\n",
      "Stored batch 43: 4300/7353 chunks\n",
      "Stored batch 44: 4400/7353 chunks\n",
      "Stored batch 45: 4500/7353 chunks\n",
      "Stored batch 44: 4400/7353 chunks\n",
      "Stored batch 45: 4500/7353 chunks\n",
      "Stored batch 46: 4600/7353 chunks\n",
      "Stored batch 47: 4700/7353 chunks\n",
      "Stored batch 48: 4800/7353 chunks\n",
      "Stored batch 46: 4600/7353 chunks\n",
      "Stored batch 47: 4700/7353 chunks\n",
      "Stored batch 48: 4800/7353 chunks\n",
      "Stored batch 49: 4900/7353 chunks\n",
      "Stored batch 50: 5000/7353 chunks\n",
      "Stored batch 49: 4900/7353 chunks\n",
      "Stored batch 50: 5000/7353 chunks\n",
      "Stored batch 51: 5100/7353 chunks\n",
      "Stored batch 52: 5200/7353 chunks\n",
      "Stored batch 53: 5300/7353 chunks\n",
      "Stored batch 51: 5100/7353 chunks\n",
      "Stored batch 52: 5200/7353 chunks\n",
      "Stored batch 53: 5300/7353 chunks\n",
      "Stored batch 54: 5400/7353 chunks\n",
      "Stored batch 55: 5500/7353 chunks\n",
      "Stored batch 56: 5600/7353 chunks\n",
      "Stored batch 54: 5400/7353 chunks\n",
      "Stored batch 55: 5500/7353 chunks\n",
      "Stored batch 56: 5600/7353 chunks\n",
      "Stored batch 57: 5700/7353 chunks\n",
      "Stored batch 58: 5800/7353 chunks\n",
      "Stored batch 59: 5900/7353 chunks\n",
      "Stored batch 57: 5700/7353 chunks\n",
      "Stored batch 58: 5800/7353 chunks\n",
      "Stored batch 59: 5900/7353 chunks\n",
      "Stored batch 60: 6000/7353 chunks\n",
      "Stored batch 61: 6100/7353 chunks\n",
      "Stored batch 62: 6200/7353 chunks\n",
      "Stored batch 60: 6000/7353 chunks\n",
      "Stored batch 61: 6100/7353 chunks\n",
      "Stored batch 62: 6200/7353 chunks\n",
      "Stored batch 63: 6300/7353 chunks\n",
      "Stored batch 64: 6400/7353 chunks\n",
      "Stored batch 65: 6500/7353 chunks\n",
      "Stored batch 63: 6300/7353 chunks\n",
      "Stored batch 64: 6400/7353 chunks\n",
      "Stored batch 65: 6500/7353 chunks\n",
      "Stored batch 66: 6600/7353 chunks\n",
      "Stored batch 67: 6700/7353 chunks\n",
      "Stored batch 68: 6800/7353 chunks\n",
      "Stored batch 66: 6600/7353 chunks\n",
      "Stored batch 67: 6700/7353 chunks\n",
      "Stored batch 68: 6800/7353 chunks\n",
      "Stored batch 69: 6900/7353 chunks\n",
      "Stored batch 70: 7000/7353 chunks\n",
      "Stored batch 69: 6900/7353 chunks\n",
      "Stored batch 70: 7000/7353 chunks\n",
      "Stored batch 71: 7100/7353 chunks\n",
      "Stored batch 72: 7200/7353 chunks\n",
      "Stored batch 73: 7300/7353 chunks\n",
      "Stored batch 74: 7353/7353 chunks\n",
      "✓ All 7353 chunks stored in Chroma.\n",
      "Stored batch 71: 7100/7353 chunks\n",
      "Stored batch 72: 7200/7353 chunks\n",
      "Stored batch 73: 7300/7353 chunks\n",
      "Stored batch 74: 7353/7353 chunks\n",
      "✓ All 7353 chunks stored in Chroma.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "def create_chroma_collection(collection_name=\"history_book\"):\n",
    "    client = chromadb.PersistentClient(path=\"../data/world_history_store\")\n",
    "\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}   # cosine distance, best for embeddings\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "def store_chunks_in_chroma(chunks, collection, embedder, batch_size=100):\n",
    "    texts = []\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        ids.append(chunk[\"chunk_id\"])\n",
    "        texts.append(chunk[\"text\"])\n",
    "\n",
    "        metadatas.append({\n",
    "            \"chapter_metadata\": chunk[\"chapter_metadata\"] if chunk[\"chapter_metadata\"] else \"UNKNOWN\",\n",
    "        })\n",
    "\n",
    "    embeddings = embedder.encode(texts).tolist()\n",
    "\n",
    "    # Add in batches to avoid ChromaDB batch size limit\n",
    "    total_chunks = len(chunks)\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "        \n",
    "        collection.add(\n",
    "            ids=ids[i:end_idx],\n",
    "            documents=texts[i:end_idx],\n",
    "            metadatas=metadatas[i:end_idx],\n",
    "            embeddings=embeddings[i:end_idx]\n",
    "        )\n",
    "        \n",
    "        print(f\"Stored batch {i//batch_size + 1}: {end_idx}/{total_chunks} chunks\")\n",
    "\n",
    "    print(f\"✓ All {total_chunks} chunks stored in Chroma.\")\n",
    "\n",
    "embedder = get_embedder()\n",
    "collection = create_chroma_collection(\"world_history\")\n",
    "\n",
    "store_chunks_in_chroma(chunks, collection, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4634e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-21\n",
      "1.6 Agriculture and the \"Neolithic Revolution\".\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-25\n",
      "Family life also changed significantly during the Neolithic.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-24\n",
      "For many scholars, the abandonment of hunting represents the \"real\" Neolithic Revolution. As communities completely abandoned hunting and\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-25\n",
      "1.6.3 Leaving Paleolithic Culture Behind While the Neolithic Era is described in greater detail elsewhere, it is important to understand Paleolithic and Neolithic differences in order to convey a sense of just how revolutionary the shift to agriculture was for humanity.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-21\n",
      "12.She and hundreds of other scholars from Hobbes to Marx have pointed to the Neolithic Revolution, that is, the move from a hunter-gatherer world to an agricultural one, as the root of what we today refer to as civilization.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-28\n",
      "About 10,000 years ago, the Neolithic Era began.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-28\n",
      "About 10,000 years ago, the Neolithic Era began.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-28\n",
      "About 10,000 years ago, the Neolithic Era began.\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-12\n",
      "How did agriculture start to change human relationships?\n",
      "—\n",
      "CHAPTER: 1 - PREHISTORY | pg-27\n",
      "These Neolithic developments in sedentary agriculture and village life would be the foundation for an explosion of cultural development three thousand years later in Egypt and Mesopotamia (addressed later in this text). By the Age of Exploration in the 1500s CE, most of the world had adopted agriculture as a primary means of subsistence, and the foundation of great civilizations.\n"
     ]
    }
   ],
   "source": [
    "def search(query, collection, embedder, k=10):\n",
    "    query_emb = embedder.encode([query]).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "r = search(\"How did the Neolithic Revolution change human societies?\", collection, embedder)\n",
    "\n",
    "for doc, meta in zip(r[\"documents\"][0], r[\"metadatas\"][0]):\n",
    "    print(\"—\")\n",
    "    print(meta[\"chapter_metadata\"])\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b304e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
